#Final project report
Alejandro de la Vega

Questionable Research Practices (QRPs) are pervasive in science, including in our field of psychology. QRPs can severely inflate the significance of results, when in fact the claimed effect is unreliable or non-existent. Simhonson highlighted a particular problem in data analysis which is termed "experimenter degrees of freedom". By manipulating these degrees of freedom one is more likely to publish a paper, but also publish unreliable effects. This manipulation is called "p-hacking".

However, such abuses of QRPs may be detected using various methods, one of which is p-curve analysis. P-curve analysis is based on the theoretical distribution of p-values, with the assumption that if there is no true effect p-values will be evenly distributed from 0-1. If there is a true effect, smaller p-values are more likely, and if there is "intense p-hacking" larger p-values closer to 0.05 are more likely to occur. Using this logic, a p-curve calculation can be done to determine if the distribution of p-values in a body of papers fits any of these patterns. Unfortunately, while this can be done for a smaller set of studies quite easily, doing so on a large-scale is unfeasible, as this requires the manual extraction of p-values from papers. Large-scale evaluation of the literature is necessary in order to identify the predictors of reproducible research. For example, it may be that certain journals or certain areas are more likely to publish reproducible research. 

With this in mind, I set out to develop a tool to automatically mine publications for p-values and conduct a large-scale p-curve analysis. The end goal would be to be able to conduct a comparative p-curve analysis on keywords that are present in the paper. The first component of this project is to extract p-values from papers. Fortunately, most journals conform to standards in reporting t and F statistics, a necessary component to calculate exact p-values. I was able to accomplish this first goal by mining journals in Frontiers of Psychology. My program automatically downloaded hundreds of papers in this journal and successfully extracted *t* and *F* values based on standard reporting patters. Manual inspection of these values confirmed that they were indeed accurate. 

Unfortunately I was not able to go further than this for a few reasons that I hope to tackle this summer. Certain journals were just technically harder to access than others and I was not able to download from non-open access journals. This problem has been solved in the past however, so I think this will just take some work to download correctly. Second, to truly automate this process I would need to write code to calculate exact p-values in my program without using the p-value web app. This is feasible but will take some time. Third, to be able to search by "keyword", I would need to mine the text of papers in order to filter papers to fit certain keyword criteria. These are all achievable goals that I hope to get to this summer.

This project also faces some larger hurdles that may not be possible to overcome. In particular, automated mining of papers will not be able to tell apart "critical" tests from "non-critical" ones. P-curve analyses traditionally require that you only input the tests that were critical for testing the papers hypothesis, as they are more likely to be p-hacked. However, I believe that while this non-selective approach will lower the power to detect p-hacking, if enough studies are collected, the overall distribution of p-values will still different across journals and fields over and above the non-critical p-values. The main confounding issues will be if different fields or journals are more or less likely to report non-critical p-values; it may be that some fields focus more on the hypothesis driven tests, while other have many "checks" that one must pass before. Nonetheless, I believe that an automated p-curve analysis would be of great use in finding potential markers of p-hacking to be later validated by more hand curated approaches. 